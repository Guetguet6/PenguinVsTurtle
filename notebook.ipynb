{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Guetguet6/PenguinVsTurtle/blob/feat/Gaetan/notebook.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5dc3028117f4f31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!rm -rf PenguinVsTurtle\n",
    "!git clone -b feat/Gaetan --single-branch https://github.com/Guetguet6/PenguinVsTurtle.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf6c72bbad12dbef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c44aad49ec85f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf data archive.zip\n",
    "!wget \"https://drive.usercontent.google.com/u/0/uc?id=1N5HskbYc1bzl-qxF2cpyT-kBq-yLxIlU&export=download\" -O archive.zip\n",
    "!unzip -q archive.zip -d data\n",
    "!mv data/train_annotations data/train_annotations.json\n",
    "!mv data/valid_annotations data/valid_annotations.json"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from skimage import io\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {},
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39cee042c7292a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618375b10ce8a43f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir: str, json_file: str, split: str = 'train', transform: torchvision.transforms = None):\n",
    "        self.root_dir: str = os.path.join(root_dir, split, split)\n",
    "        self.annotations: DataFrame = pd.read_json(os.path.join(root_dir, json_file))\n",
    "        self.transform: torchvision.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.annotations.iloc[index][\"image_id\"]\n",
    "        img_path = os.path.join(self.root_dir, f'image_id_{image_id:03d}.jpg')\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index]['category_id']))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, y_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e34f4ccf203088",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resize_imgs = 320\n",
    "transform = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.5, std=0.5), transforms.Resize(resize_imgs)])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from PenguinVsTurtle.TP5_MHA import MultiHeadAttention\n",
    "except ModuleNotFoundError:\n",
    "    from TP5_MHA import MultiHeadAttention\n",
    "\n",
    "directory = 'data'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44548c99bc4deca4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fbbe0d5e467e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(root_dir=directory, split=\"train\", json_file='train_annotations.json', transform=transform)\n",
    "valid_dataset = CustomDataset(root_dir=directory, split=\"valid\", json_file='valid_annotations.json', transform=transform)\n",
    "\n",
    "print('Number of training examples: ', len(train_dataset))\n",
    "print('Number of validation examples: ', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0f2d6715d098",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b798cf4f908f2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_idx, data = next(enumerate(train_dataloader))\n",
    "print(len(data))\n",
    "print('Images:', data[0].shape)\n",
    "b, c, h, w = data[0].shape\n",
    "print('Batch(s):', b)               # 32 images traitées à la fois\n",
    "print('Channel(s):', c)             # 3 canaux de couleur, image RGB\n",
    "print('Height:', h)                 # 320 pixels de hauteur\n",
    "print('Width:', w)                  # 320 pixels de largeur\n",
    "print('Labels:', data[1].shape)     # 32 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf0d9a10609136",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(data[0][i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(f'Label: {data[1][i]}')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e0a857f5dc415",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VisionEncoder(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, hidden_size, dropout):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Première couche : Linear + LayerNorm\n",
    "        self.norm1 = torch.nn.LayerNorm(embed_size)\n",
    "        self.mha = MultiHeadAttention(embed_size, num_heads, dropout)\n",
    "        self.add_norm = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_size, hidden_size),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, embed_size),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Première couche : Normalisation + Multi-Head Attention + Résiduel\n",
    "        norm1_output = self.norm1(x)\n",
    "        mha_output = self.mha(norm1_output, norm1_output, norm1_output)\n",
    "        add_norm1_output = x + mha_output\n",
    "\n",
    "        # Deuxième couche : Linear + GELU + Dropout + Linear + Dropout + Résiduel\n",
    "        mlp_output = self.add_norm(add_norm1_output)\n",
    "        add_norm2_output = add_norm1_output + mlp_output\n",
    "\n",
    "        return add_norm2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736be8ba5b13082",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, channel_size, patch_size, embed_size, nb_heads, classes, nb_layers, hidden_size, dropout):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Paramètres\n",
    "        self.image_size = image_size\n",
    "        self.channel_size = channel_size\n",
    "        self.pixels = image_size ** 2 * channel_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.nb_patches = (image_size // patch_size) ** 2\n",
    "        self.pixels_per_patch = channel_size * (patch_size ** 2)\n",
    "        self.nb_heads = nb_heads\n",
    "        self.classes = classes\n",
    "        self.nb_layers = nb_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # Projection linéaire\n",
    "        self.embeddings = nn.Linear(self.pixels_per_patch, self.embed_size)\n",
    "\n",
    "        # Class token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_size))\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, self.nb_patches + 1, embed_size))\n",
    "\n",
    "        # Layers d'encodage\n",
    "        self.encoders = nn.ModuleList([\n",
    "            VisionEncoder(embed_size, nb_heads, hidden_size, dropout)\n",
    "            for _ in range(nb_layers)\n",
    "        ])\n",
    "\n",
    "        # Classification\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.classifier = nn.Linear(embed_size, classes)\n",
    "\n",
    "    def forward(self, img_torch):\n",
    "        b, c, h, w = img_torch.size()\n",
    "\n",
    "        img_torch_reshape = img_torch.reshape(b, int((h / self.patch_size) * (w / self.patch_size)), c * self.patch_size * self.patch_size)\n",
    "        fwd_embeddings = self.embeddings(img_torch_reshape)\n",
    "\n",
    "        b, n, e = fwd_embeddings.size()\n",
    "        class_tokens = self.class_token.expand(b, 1, e)\n",
    "\n",
    "        concat = torch.cat((fwd_embeddings, class_tokens), dim=1)\n",
    "\n",
    "        fwd_concat = concat + self.positional_encoding\n",
    "\n",
    "        # Layout\n",
    "        fwd_dropout = self.dropout_layer(fwd_concat)\n",
    "\n",
    "        # Layers d'encodage\n",
    "        for encoder in self.encoders:\n",
    "            fwd_dropout = encoder(fwd_dropout)\n",
    "        fwd_encodeurs = fwd_dropout\n",
    "\n",
    "        # Classification\n",
    "        cls_token = fwd_encodeurs[:, -1, :]\n",
    "        cls_token = self.norm(cls_token)\n",
    "        classification = self.classifier(cls_token)\n",
    "        fwd_softmac = torch.nn.functional.log_softmax(classification, dim=1)\n",
    "        return fwd_softmac\n",
    "\n",
    "model = ViT(image_size=resize_imgs, channel_size=3, patch_size=8, embed_size=512, nb_heads=8, classes=10, nb_layers=3, hidden_size=256, dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d339c86e2d16d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fct = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Liste pour conserver les valeurs de loss (rappel : on souhaite minimiser la valeur de loss)\n",
    "losses = []\n",
    "\n",
    "# Liste pour conserver les précisions du modèle (en %)\n",
    "accuracies = []\n",
    "\n",
    "# Ces listes permettront d'afficher les courbes de loss et de précision après l'entrainement\n",
    "\n",
    "# 1 epoch correspond à un passage complet sur l'ensemble des données (les 60000 images !)\n",
    "# Le modèle va donc voir chaque image 10 fois\n",
    "nb_epochs = 15\n",
    "\n",
    "# Boucle permettant de faire nb_epochs passages sur l'ensemble des données\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "  # Passage du modèle en mode entrainement (certains paramètres agissent différemment selon si il s'agit de la phase d'entrainement ou d'évaluation)\n",
    "  model.train()\n",
    "\n",
    "  # Récupération de la loss sur l'epoch\n",
    "  epoch_loss = 0\n",
    "\n",
    "  # Liste pour conserver l'ensemble des prédictions faites durant l'epoch actuelle\n",
    "  y_pred = []\n",
    "\n",
    "  # Liste pour conserver l'ensemble des valeurs à prédire durant l'epoch actuelle\n",
    "  y_true = []\n",
    "\n",
    "  # Boucle permettant de parcourir l'ensemble des données du DataLoader (les 60000 images !)\n",
    "  # Chaque itération contient 32 images et labels comme défini lors de la création du DataLoader\n",
    "  for batch_idx, (imgs, labels) in enumerate(train_dataloader):\n",
    "    print(batch_idx)\n",
    "\n",
    "    # Envoi des données sur le processeur choisi (CPU ou GPU)\n",
    "    imgs = imgs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Passage du batch d'images dans le modèle ViT conçu\n",
    "    predictions = model(imgs)\n",
    "\n",
    "    # Calcul de la loss sur le batch\n",
    "    loss = loss_fct(predictions, labels)\n",
    "\n",
    "    # Nettoyage des anciens paramètres de mise à jour calculés\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calcul des paramètres du modèle à mettre à jour (rétropropagation du gradient)\n",
    "    loss.backward()\n",
    "\n",
    "    # Mise à jour des paramètres du modèle\n",
    "    optimizer.step()\n",
    "\n",
    "    # Mise à jour de la loss sur l'epoch\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    # La variable contient pour chaque image du batch 10 valeurs\n",
    "    # Chaque valeur correspond à une probabilité pour chacun des chiffres entre 0 et 9\n",
    "    # L'indice de la probabilité la plus forte correspond au chiffre prédit par le réseau !\n",
    "\n",
    "    # On ajoute les prédictions et les valeurs à prédire dans les listes correspondantes\n",
    "    y_pred.extend(torch.argmax(predictions, dim=1).detach().cpu().tolist())\n",
    "    y_true.extend(labels.detach().tolist())\n",
    "\n",
    "  # Ajout de la loss de l'epoch à la liste de l'ensemble des loss\n",
    "  losses.append(epoch_loss)\n",
    "\n",
    "  # Vérification et calcul de la précision du modèle en comparant pour chaque image son label avec la valeur prédite\n",
    "  nb_imgs = len(y_pred)\n",
    "  total_correct = 0\n",
    "  for i in range(nb_imgs):\n",
    "    if y_pred[i] == y_true[i]:\n",
    "      total_correct += 1\n",
    "\n",
    "  accuracy = total_correct * 100 / nb_imgs\n",
    "\n",
    "  # Ajout de la précision à la liste des précisions\n",
    "  accuracies.append(accuracy)\n",
    "\n",
    "  # Affichage des résultats pour l'epoch en cours (loss et précision)\n",
    "  print(\"----------\")\n",
    "  print(\"Epoch:\", epoch)\n",
    "  print(\"Loss:\", epoch_loss)\n",
    "  print(f\"Accuracy: {accuracy} % ({total_correct} / {nb_imgs})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ad31afb5aacbb2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train loss\")\n",
    "plt.title(\"Évolution de la Perte d’Entraînement par Epoch\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a4216b199af6ac6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(accuracies)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train accuracy (%)\")\n",
    "plt.title(\"Évolution de la Précision d’Entraînement par Epoch\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3898a4555635ac1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "\n",
    "  y_test_pred = []\n",
    "  y_test_true = []\n",
    "\n",
    "  for batch_idx, (imgs, labels) in enumerate(valid_dataloader):\n",
    "    imgs = imgs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    predictions = model(imgs)\n",
    "\n",
    "    y_test_pred.extend(predictions.detach().argmax(dim=1).tolist())\n",
    "    y_test_true.extend(labels.detach().tolist())\n",
    "\n",
    "nb_imgs = len(y_test_pred)\n",
    "total_correct = 0\n",
    "for i in range(nb_imgs):\n",
    "  if y_test_pred[i] == y_test_true[i]:\n",
    "    total_correct += 1\n",
    "\n",
    "accuracy = total_correct * 100 / nb_imgs\n",
    "\n",
    "print(f\"Evaluation accuracy: {accuracy} % ({total_correct} / {nb_imgs})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9edd244e1c12cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
